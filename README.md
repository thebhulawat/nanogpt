
# GPT-2 Implementation
This repo attempts to replicate GPT2 from scratch. I use charecter level encoding over BPR to keep things simple. 
The model is trained over input.txt which contain work of Shakespeer. 

# Add train and validation loss section
train_loss = [4.2849, 2.0147, 1.6062, 1.4424, 1.3445, 1.2808, 1.2310, 1.1866, 1.1474, 1.1092]
val_loss = [4.2824, 2.0987, 1.7879, 1.6373, 1.5687, 1.5278, 1.5050, 1.4858, 1.4793, 1.4693]


This repository contains the implementation of GPT-2 from scratch. GPT-2 is a state-of-the-art language model developed by OpenAI.

## Training Progress
### Loss
- Step 0: Train Loss 4.2849, Validation Loss 4.2824
- Step 500: Train Loss 2.0147, Validation Loss 2.0987
- Step 1000: Train Loss 1.6062, Validation Loss 1.7879
- Step 1500: Train Loss 1.4424, Validation Loss 1.6373
- Step 2000: Train Loss 1.3445, Validation Loss 1.5687
- Step 2500: Train Loss 1.2808, Validation Loss 1.5278
- Step 3000: Train Loss 1.2310, Validation Loss 1.5050
- Step 3500: Train Loss 1.1866, Validation Loss 1.4858
- Step 4000: Train Loss 1.1474, Validation Loss 1.4793
- Step 4500: Train Loss 1.1092, Validation Loss 1.4693
"""

# Thou write like Machine

Thy wring dives, baning me,--
As I find, steal'd in his voice:
Unless we to be impale! thy mischarge,
Happ'd, The-barty foul thus; thou art a
city of his lips Abret, and thy sense
Then how were such bads in hath commits the
shame wild twelve lift alloward of pity;
Where alters I not live this hour will pluckly on
Of a controm that last one cuttertainty so four!

DORSET:
Ah, to be speed; let you city out with walk,
A men that thou getting to make flaultle way.

GLORDY LOUCENTER:
Threfore will mak







